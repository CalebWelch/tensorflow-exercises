{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAL_ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Score</th>\n",
       "      <th>Synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Cowboy Bebop</td>\n",
       "      <td>8.78</td>\n",
       "      <td>In the year 2071, humanity has colonized sever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Cowboy Bebop: Tengoku no Tobira</td>\n",
       "      <td>8.39</td>\n",
       "      <td>other day, another bounty—such is the life of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Trigun</td>\n",
       "      <td>8.24</td>\n",
       "      <td>Vash the Stampede is the man with a $$60,000,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Witch Hunter Robin</td>\n",
       "      <td>7.27</td>\n",
       "      <td>ches are individuals with special powers like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Bouken Ou Beet</td>\n",
       "      <td>6.98</td>\n",
       "      <td>It is the dark century and the people are suff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MAL_ID                             Name Score  \\\n",
       "0       1                     Cowboy Bebop  8.78   \n",
       "1       5  Cowboy Bebop: Tengoku no Tobira  8.39   \n",
       "2       6                           Trigun  8.24   \n",
       "3       7               Witch Hunter Robin  7.27   \n",
       "4       8                   Bouken Ou Beet  6.98   \n",
       "\n",
       "                                            Synopsis  \n",
       "0  In the year 2071, humanity has colonized sever...  \n",
       "1  other day, another bounty—such is the life of ...  \n",
       "2  Vash the Stampede is the man with a $$60,000,0...  \n",
       "3  ches are individuals with special powers like ...  \n",
       "4  It is the dark century and the people are suff...  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animeDf = pd.read_csv('./anime_with_synopsis.csv')\n",
    "animeDf = animeDf.drop(\"Genres\",axis=1)\n",
    "animeDf = animeDf.rename(columns={\"sypnopsis\": \"Synopsis\"},)\n",
    "animeDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "honorifics = set(['Mr.', 'Mrs.', 'Ms.', 'Dr.', 'Prof.', 'Rev.', 'Capt.', 'Lt.-Col.', \n",
    "'Col.', 'Lt.-Cmdr.', 'The Hon.', 'Cmdr.', 'Flt. Lt.', 'Brgdr.', 'Wng. Cmdr.', \n",
    "'Group Capt.' ,'Rt.', 'Maj.-Gen.', 'Rear Admrl.', 'Esq.', 'Mx', 'Adv', 'Jr.'] )\n",
    "stopwords = '''a\n",
    "about\n",
    "above\n",
    "after\n",
    "again\n",
    "against\n",
    "all\n",
    "am\n",
    "an\n",
    "and\n",
    "any\n",
    "are\n",
    "aren't\n",
    "as\n",
    "at\n",
    "be\n",
    "because\n",
    "been\n",
    "before\n",
    "being\n",
    "below\n",
    "between\n",
    "both\n",
    "but\n",
    "by\n",
    "can't\n",
    "cannot\n",
    "could\n",
    "couldn't\n",
    "did\n",
    "didn't\n",
    "do\n",
    "does\n",
    "doesn't\n",
    "doing\n",
    "don't\n",
    "down\n",
    "during\n",
    "each\n",
    "few\n",
    "for\n",
    "from\n",
    "further\n",
    "had\n",
    "hadn't\n",
    "has\n",
    "hasn't\n",
    "have\n",
    "haven't\n",
    "having\n",
    "he\n",
    "he'd\n",
    "he'll\n",
    "he's\n",
    "her\n",
    "here\n",
    "here's\n",
    "hers\n",
    "herself\n",
    "him\n",
    "himself\n",
    "his\n",
    "how\n",
    "how's\n",
    "i\n",
    "i'd\n",
    "i'll\n",
    "i'm\n",
    "i've\n",
    "if\n",
    "in\n",
    "into\n",
    "is\n",
    "isn't\n",
    "it\n",
    "it's\n",
    "its\n",
    "itself\n",
    "let's\n",
    "me\n",
    "more\n",
    "most\n",
    "mustn't\n",
    "my\n",
    "myself\n",
    "no\n",
    "nor\n",
    "not\n",
    "of\n",
    "off\n",
    "on\n",
    "once\n",
    "only\n",
    "or\n",
    "other\n",
    "ought\n",
    "our\n",
    "ours\tourselves\n",
    "out\n",
    "over\n",
    "own\n",
    "same\n",
    "shan't\n",
    "she\n",
    "she'd\n",
    "she'll\n",
    "she's\n",
    "should\n",
    "shouldn't\n",
    "so\n",
    "some\n",
    "such\n",
    "than\n",
    "that\n",
    "that's\n",
    "the\n",
    "their\n",
    "theirs\n",
    "them\n",
    "themselves\n",
    "then\n",
    "there\n",
    "there's\n",
    "these\n",
    "they\n",
    "they'd\n",
    "they'll\n",
    "they're\n",
    "they've\n",
    "this\n",
    "those\n",
    "through\n",
    "to\n",
    "too\n",
    "under\n",
    "until\n",
    "up\n",
    "very\n",
    "was\n",
    "wasn't\n",
    "we\n",
    "we'd\n",
    "we'll\n",
    "we're\n",
    "we've\n",
    "were\n",
    "weren't\n",
    "what\n",
    "what's\n",
    "when\n",
    "when's\n",
    "where\n",
    "where's\n",
    "which\n",
    "while\n",
    "who\n",
    "who's\n",
    "whom\n",
    "why\n",
    "why's\n",
    "with\n",
    "won't\n",
    "would\n",
    "wouldn't\n",
    "you\n",
    "you'd\n",
    "you'll\n",
    "you're\n",
    "you've\n",
    "your\n",
    "yours\n",
    "yourself\n",
    "yourselves'''.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def removeStopWords(sent):\n",
    "    try:\n",
    "        text = [word for word in re.split(\"\\W+\",sent) if word and word.lower() not in stopwords]  # filter out empty words\n",
    "        return ' '.join(text)\n",
    "    except:\n",
    "        print(text)\n",
    "\n",
    "def splitAndStrip(sent):\n",
    "    if not pd.isnull((sent)):\n",
    "        final = []\n",
    "        split_l = sent.split(' ')\n",
    "\n",
    "        for word in split_l:\n",
    "            word = re.sub('\\'','',word)\n",
    "            word = re.sub(',',' ',word)\n",
    "            if '.' in word and word not in honorifics:\n",
    "                final.append(word + '\\n')\n",
    "                continue\n",
    "            final.append(word)\n",
    "        final = ' '.join(final).rstrip()\n",
    "        return final.split('\\n')\n",
    "    return \"\"\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')\n",
    "def processData(df):\n",
    "    x = df.Synopsis.map(lambda x: splitAndStrip(x))\n",
    "    return x\n",
    "def weighWords(para):\n",
    "    counts = {}\n",
    "    for x in para:\n",
    "        for y in x:\n",
    "            for k in y:\n",
    "                if k == '':\n",
    "                    continue\n",
    "                k = k.lower()\n",
    "                if counts.get(k) is None:\n",
    "                    counts[k] = 1\n",
    "                else:\n",
    "                    total = counts.get(k)\n",
    "                    total = total+1\n",
    "                    counts[k] = total\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vash Stampede man 60 000 000 000 bounty head', 'reason hes merciless villain lays waste oppose flattens entire cities fun garnering title Humanoid Typhoon', 'leaves trail death destruction wherever goes anyone can count dead much make eye contact rumors say', 'actuality Vash huge softie claims never taken life avoids violence costs', 'crazy doughnut obsession buffoonish attitude tow Vash traverses wasteland planet Gunsmoke followed two insurance agents Meryl Stryfe Milly Thompson attempt minimize impact public', 'soon misadventures evolve life death situations group legendary assassins summoned bring suffering trio', 'Vashs agonizing past will unraveled morality principles pushed breaking point']\n"
     ]
    }
   ],
   "source": [
    "x = processData(animeDf)\n",
    "x = x.map(lambda x: [removeStopWords(g) for g in [y for y in x]])\n",
    "# t = weighWords(x)\n",
    "# t = pd.Series(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t.where([len(x) > 3 for x in t.keys()]).dropna()\n",
    "maxFreq = t.max()\n",
    "weights = t.map(lambda x: x/maxFreq)\n",
    "sentenceDict = []\n",
    "def sumWeights(para):\n",
    "    for sent in para:\n",
    "        sentenceWeight = []\n",
    "        for word in sent:\n",
    "            w = weights.get(word.lower())\n",
    "            if w is not None:\n",
    "                sentenceWeight.append(float(w)/len(sent))\n",
    "        sentenceDict.append(round(sum(sentenceWeight), 4))\n",
    "    return sentenceDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year', '2071', 'humanity', 'colonized', 'several', 'planets', 'moons', 'solar', 'system', 'leaving', 'now', 'uninhabitable', 'surface', 'planet', 'Earth', 'behind', 'Inter', 'Solar', 'System', 'Police', 'attempts', 'keep', 'peace', 'galaxy', 'aided', 'part', 'outlaw', 'bounty', 'hunters', 'referred', 'Cowboys', 'ragtag', 'team', 'aboard', 'spaceship', 'Bebop', 'two', 'individuals', 'Mellow', 'carefree', 'Spike', 'Spiegel', 'balanced', 'boisterous', 'pragmatic', 'partner', 'Jet', 'Black', 'pair', 'makes', 'living', 'chasing', 'bounties', 'collecting', 'rewards', 'Thrown', 'course', 'addition', 'new', 'members', 'meet', 'travels', 'Ein', 'genetically', 'engineered', 'highly', 'intelligent', 'Welsh', 'Corgi', 'femme', 'fatale', 'Faye', 'Valentine', 'enigmatic', 'trickster', 'memory', 'loss', 'strange', 'computer', 'whiz', 'kid', 'Edward', 'Wong', 'crew', 'embarks', 'thrilling', 'adventures', 'unravel', 'member', 's', 'dark', 'mysterious', 'past', 'little', 'little', 'Well', 'balanced', 'high', 'density', 'action', 'light', 'hearted', 'comedy', 'Cowboy', 'Bebop', 'space', 'Western', 'classic', 'homage', 'smooth', 'improvised', 'music', 'named']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "text = flatten_list(x[0])\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(text)\n",
    "vectorizer2 = TfidfVectorizer()\n",
    "vectorizer2.fit(text)\n",
    "hashv = HashingVectorizer(n_features=20)\n",
    "v = hashv.transform(text)\n",
    "v.toarray()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf_gpu_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd4f6dceff4c78247da923d5344dc3e0de566f821ad1433ca8f33e011d09153b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
